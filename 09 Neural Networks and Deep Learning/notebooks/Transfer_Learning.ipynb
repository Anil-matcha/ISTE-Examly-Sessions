{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anil-matcha/Speaking-Engagements/blob/master/Tf_lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTa3Ee15WsJ"
      },
      "source": [
        "# Recognize Images using Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMcobPHdD8O"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # The %tensorflow_version magic only works in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOG3l_MsBO1A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cd858e8c-d98c-4bab-8b71-2815b147d245"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v77rlkCKW0IJ"
      },
      "source": [
        "## Setup Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4QOy2uA3P_p"
      },
      "source": [
        "Download the dataset from online."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM51yVFz51DU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "77ab14c7-b35c-45e0-97e2-8935d2191649"
      },
      "source": [
        "!pip install google_images_download"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_images_download\n",
            "  Downloading https://files.pythonhosted.org/packages/18/ed/0319d30c48f3653802da8e6dcfefcea6370157d10d566ef6807cceb5ec4d/google_images_download-2.8.0.tar.gz\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->google_images_download) (1.24.3)\n",
            "Building wheels for collected packages: google-images-download\n",
            "  Building wheel for google-images-download (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-images-download: filename=google_images_download-2.8.0-py2.py3-none-any.whl size=14550 sha256=118c71a37f5266adc4d86187955644bee4e8082666b0b1bdf4b1ed0de1d61f7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/28/ad/f56e7061e1d2a9a1affe2f9c649c2570cb9198dd24ede0bbab\n",
            "Successfully built google-images-download\n",
            "Installing collected packages: selenium, google-images-download\n",
            "Successfully installed google-images-download-2.8.0 selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxL2mjVVGIrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "ddd4dc77-0520-4266-b63f-70670e5dfb46"
      },
      "source": [
        "# importing google_images_download module \n",
        "from google_images_download import google_images_download  \n",
        "  \n",
        "# creating object \n",
        "response = google_images_download.googleimagesdownload()  \n",
        "  \n",
        "search_queries = ['human', 'dog', 'cat', 'horse'] \n",
        "  \n",
        "  \n",
        "def downloadimages(query): \n",
        "    # keywords is the search query \n",
        "    # format is the image file format \n",
        "    # limit is the number of images to be downloaded \n",
        "    # print urs is to print the image file url \n",
        "    # size is the image size which can \n",
        "    # be specified manually (\"large, medium, icon\") \n",
        "    # aspect ratio denotes the height width ratio \n",
        "    # of images to download. (\"tall, square, wide, panoramic\") \n",
        "    arguments = {\"keywords\": query, \n",
        "                 \"format\": \"jpg\", \n",
        "                 \"limit\":50, \n",
        "                 \"print_urls\":True, \n",
        "                 \"size\": \"medium\", \n",
        "                 \"aspect_ratio\": \"panoramic\"} \n",
        "    try: \n",
        "        response.download(arguments) \n",
        "      \n",
        "    # Handling File NotFound Error     \n",
        "    except FileNotFoundError:  \n",
        "        arguments = {\"keywords\": query, \n",
        "                     \"format\": \"jpg\", \n",
        "                     \"limit\":4, \n",
        "                     \"print_urls\":True,  \n",
        "                     \"size\": \"medium\"} \n",
        "                       \n",
        "        # Providing arguments for the searched query \n",
        "        try: \n",
        "            # Downloading the photos based \n",
        "            # on the given arguments \n",
        "            response.download(arguments)  \n",
        "        except: \n",
        "            pass\n",
        "  \n",
        "# Driver Code \n",
        "for query in search_queries: \n",
        "    downloadimages(query)  \n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Item no.: 1 --> Item name = human\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "\n",
            "\n",
            "Unfortunately all 50 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
            "\n",
            "Errors: 0\n",
            "\n",
            "\n",
            "\n",
            "Item no.: 1 --> Item name = dog\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "\n",
            "\n",
            "Unfortunately all 50 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
            "\n",
            "Errors: 0\n",
            "\n",
            "\n",
            "\n",
            "Item no.: 1 --> Item name = cat\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "\n",
            "\n",
            "Unfortunately all 50 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
            "\n",
            "Errors: 0\n",
            "\n",
            "\n",
            "\n",
            "Item no.: 1 --> Item name = horse\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "\n",
            "\n",
            "Unfortunately all 50 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
            "\n",
            "Errors: 0\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJUCnHzdavMu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "9c55ec07-cc74-474d-fa92-0a7af3558f64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhpW-GHHx-Zz"
      },
      "source": [
        "!rm -rf dataset\n",
        "!cp /content/drive/'My Drive'/dataset.zip .\n",
        "!unzip -qq dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FSTdd98Wgf"
      },
      "source": [
        "import os\n",
        "from PIL import Image as pil_image\n",
        "directory = \"dataset/dog\"\n",
        "#[os.rename(os.path.join(directory, f), os.path.join(directory, f).replace(' ', '_').replace('(', '').replace(')', '').lower()) for f in os.listdir(directory)]\n",
        "folders = os.listdir(\"dataset\")\n",
        "for folder in folders:\n",
        "  images = os.listdir(\"dataset/\"+folder)\n",
        "  for image in images:\n",
        "    image_name = \"dataset/\"+folder+\"/\"+image\n",
        "    if \"%20\" in image:\n",
        "      new_image_name = image_name.replace(\"%20\", \"\")\n",
        "      os.rename(image_name, new_image_name)\n",
        "      image_name = new_image_name\n",
        "    try:\n",
        "      pil_image.open(image_name)\n",
        "    except:\n",
        "      os.remove(image_name)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4gTv7ig2vMh"
      },
      "source": [
        "Use `ImageDataGenerator` to rescale the images.\n",
        "\n",
        "Create the train generator and specify where the train dataset directory, image size, batch size.\n",
        "\n",
        "Create the validation generator with similar approach as the train generator with the flow_from_directory() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCLb_yV5JfF3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c02da876-dd01-42c0-ec5f-86b17d61ea5d"
      },
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "base_dir = \"dataset\"\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    validation_split=0.2,\n",
        "\t\thorizontal_flip=True,\n",
        "\t\tfill_mode=\"nearest\")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE, \n",
        "    subset='training')\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE, \n",
        "    subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 269 images belonging to 3 classes.\n",
            "Found 66 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx1L7fxxWA_G"
      },
      "source": [
        "for image_batch, label_batch in train_generator:\n",
        "  break\n",
        "image_batch.shape, label_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrFFcwUb3iK9"
      },
      "source": [
        "Save the labels in a file which will be downloaded later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QFZIhWs4dsq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c39f22a-1f4a-4fe3-96ac-bd0da8094463"
      },
      "source": [
        "print (train_generator.class_indices)\n",
        "\n",
        "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'cat': 0, 'dog': 1, 'human': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duxD_UDSOmng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1603d0c1-16e9-44d3-fd3c-dce8a4ea9eb8"
      },
      "source": [
        "!cat labels.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\n",
            "dog\n",
            "human"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkH-kazQecHB"
      },
      "source": [
        "## Create the base model from the pre-trained convnets\n",
        "\n",
        "Create the base model from the **MobileNet V2** model developed at Google, and pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\n",
        "\n",
        "First, pick which intermediate layer of MobileNet V2 will be used for feature extraction. A common practice is to use the output of the very last layer before the flatten operation, the so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers will be too specialized to the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
        "\n",
        "Let's instantiate an MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the `include_top=False` argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19IQ2gqneqmS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "404eec7a-834c-47e1-b9e5-796a94e874cd"
      },
      "source": [
        "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                              include_top=False, \n",
        "                                              weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlx56nQtfe8Y"
      },
      "source": [
        "## Feature extraction\n",
        "You will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tts8BbAtRGvk"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdMRM8YModbk"
      },
      "source": [
        "### Add a classification head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eApvroIyn1K0"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ylJXE_kRLi"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpR8HdyMhukJ"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ARiyMFsgbH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "9bec08e8-8972-40ab-c1d8-408ffd4c8032"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 5, 5, 32)          368672    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 2,626,755\n",
            "Trainable params: 368,771\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krvBumovycVA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9aade519-a70e-4924-d2e6-66b1d0a341e9"
      },
      "source": [
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable variables = 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxvgOYTDSWTx"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsaRFlZ9B6WK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "75c14663-6cf3-402d-9faf-7c5f246899d4"
      },
      "source": [
        "epochs = 10\n",
        "!mkdir training\n",
        "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "history = model.fit_generator(train_generator, \n",
        "                    epochs=epochs, \n",
        "                    validation_data=val_generator,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘training’: File exists\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 3.5253e-04 - accuracy: 1.0000\n",
            "Epoch 00001: saving model to training/cp-0001.ckpt\n",
            "9/9 [==============================] - 1s 96ms/step - loss: 3.5253e-04 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 3.7505e-04 - accuracy: 1.0000\n",
            "Epoch 00002: saving model to training/cp-0002.ckpt\n",
            "9/9 [==============================] - 1s 90ms/step - loss: 3.7505e-04 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 2.8668e-04 - accuracy: 1.0000\n",
            "Epoch 00003: saving model to training/cp-0003.ckpt\n",
            "9/9 [==============================] - 1s 90ms/step - loss: 2.8668e-04 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 0.9848\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 3.3303e-04 - accuracy: 1.0000\n",
            "Epoch 00004: saving model to training/cp-0004.ckpt\n",
            "9/9 [==============================] - 1s 92ms/step - loss: 3.3303e-04 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 0.9848\n",
            "Epoch 5/10\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 2.6957e-04 - accuracy: 1.0000\n",
            "Epoch 00005: saving model to training/cp-0005.ckpt\n",
            "9/9 [==============================] - 1s 89ms/step - loss: 2.7565e-04 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 0.9697\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 2.5168e-04 - accuracy: 1.0000\n",
            "Epoch 00006: saving model to training/cp-0006.ckpt\n",
            "9/9 [==============================] - 1s 100ms/step - loss: 2.5168e-04 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 0.9848\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 2.3295e-04 - accuracy: 1.0000\n",
            "Epoch 00007: saving model to training/cp-0007.ckpt\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 2.3295e-04 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 0.9848\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 2.3441e-04 - accuracy: 1.0000\n",
            "Epoch 00008: saving model to training/cp-0008.ckpt\n",
            "9/9 [==============================] - 1s 92ms/step - loss: 2.3441e-04 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 0.9848\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 2.9685e-04 - accuracy: 1.0000\n",
            "Epoch 00009: saving model to training/cp-0009.ckpt\n",
            "9/9 [==============================] - 1s 91ms/step - loss: 2.9685e-04 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 0.9697\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5202e-04 - accuracy: 1.0000\n",
            "Epoch 00010: saving model to training/cp-0010.ckpt\n",
            "9/9 [==============================] - 1s 90ms/step - loss: 1.5202e-04 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 0.9848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqwV-CRdS6Nv"
      },
      "source": [
        "## Fine tuning\n",
        "In our feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were **not** updated during training.\n",
        "\n",
        " One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPXnzUK0QonF"
      },
      "source": [
        "### Un-freeze the top layers of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxv_ifotQak"
      },
      "source": [
        "All you need to do is unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nzcagVitLQm"
      },
      "source": [
        "base_model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4HgVAacRs5v"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uk1dgsxT0IS"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "Compile the model using a much lower training rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtUnaz0WUDva"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(1e-5),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwBWy7J2kZvA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNXelbMQtonr"
      },
      "source": [
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5O4jd6TuAG"
      },
      "source": [
        "### Continue Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECQLkAsFTlun"
      },
      "source": [
        "history_fine = model.fit_generator(train_generator, \n",
        "                         epochs=5,\n",
        "                         validation_data=val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TZTwG7nhm0C"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
        "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
        "\n",
        "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
        "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTgGloS1e7G6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "24a50ffa-0c13-4a33-e77c-68f11d4449ad"
      },
      "source": [
        "saved_model_dir = 'save/fine_tuning'\n",
        "tf.saved_model.save(model, saved_model_dir)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: save/fine_tuning/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj6UGQeFe7zl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9abfb059-4ef8-481d-d97f-71e8b6ec697d"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model.tflite')\n",
        "files.download('labels.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_71ad7cbc-6bae-4c5e-b1e4-c305418b59f6\", \"model.tflite\", 10378484)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e50dc6ca-1faf-4f10-ab3b-577531ff0911\", \"labels.txt\", 13)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}